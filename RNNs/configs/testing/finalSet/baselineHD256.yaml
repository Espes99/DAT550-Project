test_flight: false

preprocess:
  max_len: 350

training:
  lr: 0.01

batch_size: 8

model:
  rnn_type: "lstm"
  bidirectional: false
  attention_layer: "none"
  return_attn_weights: true
  num_layers: 1
  dropout: 0.1
  hidden_dim: 16

embedding:
  embedding_dim: 50
  embedding_type: "glove"
  freeze: true

model_dir: outputs/training/baselineHD256/baselineHD256
log_dir: outputs/training/baselineHD256/baselineHD256

dir_extention: baselineHD256 # testGruBiCustomGloveFSched testLstmNonbiCustomDotGloveFSched testLstmNonbiNoneGloveFSched